{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14557785,"sourceType":"datasetVersion","datasetId":9298365}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Crowd Head Detection + Counting (YOLOv8n)\n\nGoal: fine-tune a YOLOv8 nano detector on the **RPEE-Heads** dataset, then use detections to estimate **crowd size (heads per frame)**.\n\nThis notebook is designed to run on **Kaggle GPU**.\n","metadata":{}},{"cell_type":"code","source":"!pip -q install ultralytics\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom ultralytics import __version__ as ultralytics_version\n\nprint('ultralytics:', ultralytics_version)\nprint('torch:', torch.__version__)\nprint('cuda available:', torch.cuda.is_available())\nif torch.cuda.is_available():\n    print('gpu:', torch.cuda.get_device_name(0))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nDATA_ROOT = \"/kaggle/input/2024rpee-heads-dataset\"\nprint(DATA_ROOT)\nprint(os.listdir(DATA_ROOT))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nfrom pathlib import Path\n\nROOT = \"/kaggle/input/2024rpee-heads-dataset\"\n\ndef find_images_dir(split_folder: str) -> str:\n    # find any \".../<split_folder>/**/images\" that also has a sibling \"labels\"\n    candidates = glob.glob(f\"{ROOT}/{split_folder}/**/images\", recursive=True)\n    for img_dir in candidates:\n        lab_dir = img_dir.replace(\"/images\", \"/labels\")\n        if os.path.isdir(img_dir) and os.path.isdir(lab_dir):\n            # return relative path from ROOT\n            return os.path.relpath(img_dir, ROOT)\n    raise FileNotFoundError(f\"Could not find an images/labels pair under: {ROOT}/{split_folder}\")\n\ntrain_rel = find_images_dir(\"training\")\nval_rel   = find_images_dir(\"validation\")\ntest_rel  = find_images_dir(\"testing\")\n\nprint(\"Detected:\")\nprint(\" train:\", train_rel)\nprint(\" val  :\", val_rel)\nprint(\" test :\", test_rel)\n\nyaml_path = Path(\"/kaggle/working/rpee_heads.yaml\")\nyaml_path.write_text(f\"\"\"\\\npath: {ROOT}\n\ntrain: {train_rel}\nval: {val_rel}\ntest: {test_rel}\n\nnames: [head]\nnc: 1\n\"\"\")\nprint(\"\\nWrote YAML:\\n\", yaml_path.read_text())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!yolo detect train model=yolov8n.pt data=/kaggle/working/rpee_heads.yaml imgsz=832 epochs=80 batch=16 device=0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate on the test split","metadata":{}},{"cell_type":"code","source":"!yolo detect val model=/kaggle/working/runs/detect/train/weights/best.pt data=/kaggle/working/rpee_heads.yaml split=test imgsz=832\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Counting evaluation (MAE/RMSE of head counts)","metadata":{}},{"cell_type":"markdown","source":"## Counting metrics (what we measure)\n\nFor each image/frame:\n- **GT count** = number of labeled head boxes in the YOLO label file\n- **Pred count** = number of predicted boxes after confidence filtering + NMS\n- **Error** = `pred - gt`\n\nWe report:\n- **MAE**: average absolute error (\"on average we miss/overcount by X heads\")\n- **RMSE**: penalizes large errors more strongly\n- **Bias**: average signed error (negative = undercount, positive = overcount)\n\nWe also compute **bucketed errors** by crowd density (0–10, 11–30, …).\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nPath(\"/kaggle/working/eval_counting.py\").write_text(r\"\"\"\nimport os\nimport glob\nimport math\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nfrom ultralytics import YOLO\n\ndef load_yolo_labels(txt_path: Path) -> int:\n    if not txt_path.exists():\n        return 0\n    lines = [ln.strip() for ln in txt_path.read_text().splitlines() if ln.strip()]\n    return len(lines)\n\ndef get_split_dirs(data_root: str, split: str):\n    # data_root is YAML path root, split folder is e.g. 'testing' etc and maybe nested\n    # We'll locate the first images dir that has sibling labels.\n    candidates = glob.glob(os.path.join(data_root, split, \"**\", \"images\"), recursive=True)\n    for img_dir in candidates:\n        lab_dir = img_dir.replace(\"/images\", \"/labels\")\n        if os.path.isdir(img_dir) and os.path.isdir(lab_dir):\n            return Path(img_dir), Path(lab_dir)\n    raise FileNotFoundError(f\"Could not find images/labels under {data_root}/{split}\")\n\ndef eval_counts(model_path: str, data_root: str, split_folder: str, imgsz: int = 832, conf: float = 0.25, iou: float = 0.7, max_det: int = 300):\n    img_dir, lab_dir = get_split_dirs(data_root, split_folder)\n\n    model = YOLO(model_path)\n\n    image_paths = sorted([p for p in img_dir.glob(\"*\") if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]])\n    rows = []\n\n    # batch inference for speed\n    results = model.predict(\n        source=[str(p) for p in image_paths],\n        imgsz=imgsz,\n        conf=conf,\n        iou=iou,\n        max_det=max_det,\n        device=0,\n        verbose=False,\n        stream=False,\n    )\n\n    for p, r in zip(image_paths, results):\n        gt = load_yolo_labels(lab_dir / (p.stem + \".txt\"))\n        pred = 0 if r.boxes is None else int(r.boxes.shape[0])\n        rows.append({\"image\": p.name, \"gt_count\": gt, \"pred_count\": pred, \"error\": pred - gt})\n\n    df = pd.DataFrame(rows)\n    mae = float(np.mean(np.abs(df[\"error\"])))\n    rmse = float(math.sqrt(np.mean(df[\"error\"] ** 2)))\n    bias = float(np.mean(df[\"error\"]))\n\n    # bucketed errors by crowd level\n    bins = [-1, 10, 30, 60, 100, 200, 10**9]\n    labels = [\"0-10\", \"11-30\", \"31-60\", \"61-100\", \"101-200\", \"200+\"]\n    df[\"gt_bucket\"] = pd.cut(df[\"gt_count\"], bins=bins, labels=labels)\n    bucket = df.groupby(\"gt_bucket\", observed=True).agg(\n        n=(\"image\", \"count\"),\n        mae=(\"error\", lambda x: float(np.mean(np.abs(x)))),\n        rmse=(\"error\", lambda x: float(math.sqrt(np.mean(x**2)))),\n        bias=(\"error\", lambda x: float(np.mean(x))),\n    ).reset_index()\n\n    summary = {\n        \"split\": split_folder,\n        \"n_images\": int(len(df)),\n        \"mae\": mae,\n        \"rmse\": rmse,\n        \"bias\": bias,\n        \"conf\": conf,\n        \"iou\": iou,\n        \"imgsz\": imgsz,\n        \"max_det\": max_det,\n        \"img_dir\": str(img_dir),\n    }\n    return df, bucket, summary\n\ndef main():\n    MODEL = \"/kaggle/working/runs/detect/train/weights/best.pt\"\n    DATA_ROOT = \"/kaggle/input/2024rpee-heads-dataset\"\n    OUT_DIR = Path(\"/kaggle/working/count_eval\")\n    OUT_DIR.mkdir(parents=True, exist_ok=True)\n\n    # Tune conf later; start with default 0.25\n    for split in [\"validation\", \"testing\"]:\n        df, bucket, summary = eval_counts(MODEL, DATA_ROOT, split_folder=split, imgsz=832, conf=0.25, iou=0.7, max_det=300)\n\n        pd.DataFrame([summary]).to_csv(OUT_DIR / f\"summary_{split}.csv\", index=False)\n        df.to_csv(OUT_DIR / f\"per_image_{split}.csv\", index=False)\n        bucket.to_csv(OUT_DIR / f\"bucket_{split}.csv\", index=False)\n\n        print(summary)\n        print(bucket)\n\nif __name__ == \"__main__\":\n    main()\n\"\"\")\n\n!python /kaggle/working/eval_counting.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scatter: predicted vs GT count + error histogram","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"/kaggle/working/count_eval/per_image_testing.csv\")\n\n# Scatter\nplt.figure()\nplt.scatter(df[\"gt_count\"], df[\"pred_count\"], s=6)\nplt.xlabel(\"Ground truth head count\")\nplt.ylabel(\"Predicted head count\")\nplt.title(\"Head counting: Predicted vs Ground Truth (test)\")\nplt.savefig(\"/kaggle/working/count_eval/scatter_test.png\", dpi=200, bbox_inches=\"tight\")\nplt.close()\n\n# Error histogram\nplt.figure()\nplt.hist(df[\"error\"], bins=60)\nplt.xlabel(\"Prediction error (pred - gt)\")\nplt.ylabel(\"Number of images\")\nplt.title(\"Head counting error distribution (test)\")\nplt.savefig(\"/kaggle/working/count_eval/error_hist_test.png\", dpi=200, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"Saved plots to /kaggle/working/count_eval/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tune conf for best counting MAE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ultralytics import YOLO\nimport os, glob, math\nfrom pathlib import Path\n\nMODEL = YOLO(\"/kaggle/working/runs/detect/train/weights/best.pt\")\nDATA_ROOT = \"/kaggle/input/2024rpee-heads-dataset\"\n\n# locate validation images/labels\nval_imgs = glob.glob(f\"{DATA_ROOT}/validation/**/images/*.*\", recursive=True)\nval_imgs = [p for p in val_imgs if p.lower().endswith((\".jpg\",\".jpeg\",\".png\"))]\nval_imgs = sorted(val_imgs)\n\nval_labels_dir = None\n# find sibling labels dir of first image\nfor img in val_imgs[:1]:\n    d = os.path.dirname(img)\n    cand = d.replace(\"/images\", \"/labels\")\n    if os.path.isdir(cand):\n        val_labels_dir = cand\n        break\nassert val_labels_dir is not None\n\ndef gt_count(img_path):\n    txt = os.path.join(val_labels_dir, Path(img_path).stem + \".txt\")\n    if not os.path.exists(txt):\n        return 0\n    with open(txt, \"r\") as f:\n        return sum(1 for _ in f if _.strip())\n\ngt = np.array([gt_count(p) for p in val_imgs], dtype=int)\n\ndef eval_conf(conf):\n    res = MODEL.predict(val_imgs, imgsz=832, conf=conf, iou=0.7, device=0, verbose=False)\n    pred = np.array([0 if r.boxes is None else int(r.boxes.shape[0]) for r in res], dtype=int)\n    err = pred - gt\n    mae = float(np.mean(np.abs(err)))\n    bias = float(np.mean(err))\n    rmse = float(math.sqrt(np.mean(err**2)))\n    return mae, rmse, bias\n\ngrid = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35]\nrows=[]\nfor c in grid:\n    mae, rmse, bias = eval_conf(c)\n    rows.append({\"conf\": c, \"mae\": mae, \"rmse\": rmse, \"bias\": bias})\ndf = pd.DataFrame(rows).sort_values(\"mae\")\ndf.to_csv(\"/kaggle/working/count_eval/conf_sweep_val.csv\", index=False)\ndf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## NMS IoU sweep","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ultralytics import YOLO\nimport os, glob, math\nfrom pathlib import Path\n\nMODEL = YOLO(\"/kaggle/working/runs/detect/train/weights/best.pt\")\nDATA_ROOT = \"/kaggle/input/2024rpee-heads-dataset\"\n\nval_imgs = sorted([p for p in glob.glob(f\"{DATA_ROOT}/validation/**/images/*.*\", recursive=True)\n                   if p.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n\nval_labels_dir = None\nfor img in val_imgs[:1]:\n    cand = os.path.dirname(img).replace(\"/images\", \"/labels\")\n    if os.path.isdir(cand):\n        val_labels_dir = cand\n        break\nassert val_labels_dir is not None\n\ndef gt_count(img_path):\n    txt = os.path.join(val_labels_dir, Path(img_path).stem + \".txt\")\n    if not os.path.exists(txt): return 0\n    with open(txt, \"r\") as f:\n        return sum(1 for ln in f if ln.strip())\n\ngt = np.array([gt_count(p) for p in val_imgs], dtype=int)\n\ndef eval_iou(iou, conf=0.25):\n    res = MODEL.predict(val_imgs, imgsz=832, conf=conf, iou=iou, device=0, verbose=False)\n    pred = np.array([0 if r.boxes is None else int(r.boxes.shape[0]) for r in res], dtype=int)\n    err = pred - gt\n    mae = float(np.mean(np.abs(err)))\n    bias = float(np.mean(err))\n    rmse = float(math.sqrt(np.mean(err**2)))\n    return mae, rmse, bias\n\ngrid = [0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75]\nrows = []\nfor i in grid:\n    mae, rmse, bias = eval_iou(i)\n    rows.append({\"iou\": i, \"conf\": 0.25, \"mae\": mae, \"rmse\": rmse, \"bias\": bias})\n\ndf = pd.DataFrame(rows).sort_values(\"mae\")\ndf.to_csv(\"/kaggle/working/count_eval/iou_sweep_val.csv\", index=False)\ndf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compare bucket errors at iou=0.70 vs 0.75\n\n real pain is the 101–200 bucket. The global MAE can hide whether 0.75 helps that bucket.\n\nRun this for both val and test and compare only the dense bucket:","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport pandas as pd\nimport numpy as np\nimport os, glob, math\nfrom pathlib import Path\n\nMODEL_PATH = \"/kaggle/working/runs/detect/train/weights/best.pt\"\nDATA_ROOT  = \"/kaggle/input/2024rpee-heads-dataset\"\n\ndef find_pair(split_folder):\n    imgs = glob.glob(f\"{DATA_ROOT}/{split_folder}/**/images/*.*\", recursive=True)\n    imgs = sorted([p for p in imgs if p.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n    for p in imgs[:1]:\n        lab = os.path.dirname(p).replace(\"/images\", \"/labels\")\n        if os.path.isdir(lab):\n            return imgs, lab\n    raise FileNotFoundError(split_folder)\n\ndef gt_count(labels_dir, img_path):\n    txt = os.path.join(labels_dir, Path(img_path).stem + \".txt\")\n    if not os.path.exists(txt): return 0\n    with open(txt, \"r\") as f:\n        return sum(1 for ln in f if ln.strip())\n\ndef bucket_eval(split_folder, conf, iou):\n    imgs, lab_dir = find_pair(split_folder)\n    model = YOLO(MODEL_PATH)\n    gt = np.array([gt_count(lab_dir, p) for p in imgs], dtype=int)\n\n    res = model.predict(imgs, imgsz=832, conf=conf, iou=iou, max_det=300, device=0, verbose=False)\n    pred = np.array([0 if r.boxes is None else int(r.boxes.shape[0]) for r in res], dtype=int)\n\n    err = pred - gt\n    df = pd.DataFrame({\"gt\": gt, \"pred\": pred, \"err\": err})\n\n    bins = [-1, 10, 30, 60, 100, 200, 10**9]\n    labels = [\"0-10\", \"11-30\", \"31-60\", \"61-100\", \"101-200\", \"200+\"]\n    df[\"bucket\"] = pd.cut(df[\"gt\"], bins=bins, labels=labels)\n\n    out = df.groupby(\"bucket\", observed=True).agg(\n        n=(\"gt\", \"count\"),\n        mae=(\"err\", lambda x: float(np.mean(np.abs(x)))),\n        rmse=(\"err\", lambda x: float(math.sqrt(np.mean(x**2)))),\n        bias=(\"err\", lambda x: float(np.mean(x))),\n    ).reset_index()\n\n    overall = {\n        \"split\": split_folder, \"conf\": conf, \"iou\": iou,\n        \"mae\": float(np.mean(np.abs(err))),\n        \"rmse\": float(math.sqrt(np.mean(err**2))),\n        \"bias\": float(np.mean(err)),\n    }\n    return pd.DataFrame([overall]), out\n\nfor iou in [0.70, 0.75]:\n    for split in [\"validation\", \"testing\"]:\n        overall, bucket = bucket_eval(split, conf=0.25, iou=iou)\n        print(\"\\n\", overall.to_string(index=False))\n        print(bucket.to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calibration of counts","metadata":{}},{"cell_type":"markdown","source":"## Calibration experiment (and why we keep it as analysis, not as production)\n\nWe tried **isotonic regression** to map `pred_count -> corrected_count` using the validation set.\n\n- It **improved validation MAE** (because it can overfit the validation distribution).\n- It **hurt test MAE** and shifted error between buckets (classic \"calibration overfit\" / distribution shift).\n\nConclusion for a robust baseline:\n- Prefer tuning **confidence threshold** and **NMS IoU**\n- If we calibrate, we do it with stronger regularization and/or more representative data from the target cameras\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.isotonic import IsotonicRegression\n\nval  = pd.read_csv(\"/kaggle/working/count_eval/per_image_validation.csv\")\ntest = pd.read_csv(\"/kaggle/working/count_eval/per_image_testing.csv\")\n\n# Fit on validation only\niso = IsotonicRegression(out_of_bounds=\"clip\")\niso.fit(val[\"pred_count\"].values, val[\"gt_count\"].values)\n\n# Apply (rounded to integer counts, no negatives)\nval[\"pred_count_cal\"]  = np.maximum(0, np.rint(iso.predict(val[\"pred_count\"].values)).astype(int))\ntest[\"pred_count_cal\"] = np.maximum(0, np.rint(iso.predict(test[\"pred_count\"].values)).astype(int))\n\nval.to_csv(\"/kaggle/working/count_eval/per_image_validation_calibrated.csv\", index=False)\ntest.to_csv(\"/kaggle/working/count_eval/per_image_testing_calibrated.csv\", index=False)\n\nprint(\"Saved calibrated CSVs.\")\nprint(\"val columns:\", list(val.columns))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\n\ndef metrics(df, pred_col):\n    err = df[pred_col].values - df[\"gt_count\"].values\n    return {\n        \"mae\": float(np.mean(np.abs(err))),\n        \"rmse\": float(math.sqrt(np.mean(err**2))),\n        \"bias\": float(np.mean(err))\n    }\n\nval  = pd.read_csv(\"/kaggle/working/count_eval/per_image_validation_calibrated.csv\")\ntest = pd.read_csv(\"/kaggle/working/count_eval/per_image_testing_calibrated.csv\")\n\nrows = []\nrows.append({\"split\":\"validation\",\"version\":\"raw\", **metrics(val, \"pred_count\")})\nrows.append({\"split\":\"validation\",\"version\":\"cal\", **metrics(val, \"pred_count_cal\")})\nrows.append({\"split\":\"testing\",\"version\":\"raw\", **metrics(test, \"pred_count\")})\nrows.append({\"split\":\"testing\",\"version\":\"cal\", **metrics(test, \"pred_count_cal\")})\n\npd.DataFrame(rows)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\ndef bucket_table(df, pred_col):\n    gt = df[\"gt_count\"].values\n    pred = df[pred_col].values\n    err = pred - gt\n\n    bins = [-1, 10, 30, 60, 100, 200, 10**9]\n    labels = [\"0-10\", \"11-30\", \"31-60\", \"61-100\", \"101-200\", \"200+\"]\n    b = pd.cut(gt, bins=bins, labels=labels)\n\n    out = pd.DataFrame({\"bucket\": b, \"err\": err}).groupby(\"bucket\", observed=True).agg(\n        n=(\"err\",\"count\"),\n        mae=(\"err\", lambda x: float(np.mean(np.abs(x)))),\n        rmse=(\"err\", lambda x: float(math.sqrt(np.mean(x**2)))),\n        bias=(\"err\", lambda x: float(np.mean(x))),\n    ).reset_index()\n    return out\n\nval  = pd.read_csv(\"/kaggle/working/count_eval/per_image_validation_calibrated.csv\")\ntest = pd.read_csv(\"/kaggle/working/count_eval/per_image_testing_calibrated.csv\")\n\nprint(\"TEST raw buckets\")\nprint(bucket_table(test, \"pred_count\").to_string(index=False))\nprint(\"\\nTEST calibrated buckets\")\nprint(bucket_table(test, \"pred_count_cal\").to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nfrom sklearn.isotonic import IsotonicRegression\n\nMAX_DET = 300  # must match the inference max_det\n\nval  = pd.read_csv(\"/kaggle/working/count_eval/per_image_validation.csv\")\ntest = pd.read_csv(\"/kaggle/working/count_eval/per_image_testing.csv\")\n\nx = val[\"pred_count\"].values.astype(float)\ny = val[\"gt_count\"].values.astype(float)\n\n# Anchors to make extrapolation sane\nx_aug = np.concatenate([x, [0.0, float(MAX_DET)]])\ny_aug = np.concatenate([y, [0.0, float(MAX_DET)]])\n\niso = IsotonicRegression(out_of_bounds=\"clip\")\niso.fit(x_aug, y_aug)\n\ndef apply_iso(df):\n    cal = iso.predict(df[\"pred_count\"].values.astype(float))\n    df = df.copy()\n    df[\"pred_count_cal\"] = np.clip(np.rint(cal), 0, MAX_DET).astype(int)\n    return df\n\nval_cal  = apply_iso(val)\ntest_cal = apply_iso(test)\n\nval_cal.to_csv(\"/kaggle/working/count_eval/per_image_validation_calibrated_anchored.csv\", index=False)\ntest_cal.to_csv(\"/kaggle/working/count_eval/per_image_testing_calibrated_anchored.csv\", index=False)\n\ndef metrics(df, pred_col):\n    err = df[pred_col].values - df[\"gt_count\"].values\n    return {\n        \"mae\": float(np.mean(np.abs(err))),\n        \"rmse\": float(math.sqrt(np.mean(err**2))),\n        \"bias\": float(np.mean(err)),\n    }\n\nsummary = pd.DataFrame([\n    {\"split\":\"validation\",\"version\":\"raw\", **metrics(val, \"pred_count\")},\n    {\"split\":\"validation\",\"version\":\"cal_anchored\", **metrics(val_cal, \"pred_count_cal\")},\n    {\"split\":\"testing\",\"version\":\"raw\", **metrics(test, \"pred_count\")},\n    {\"split\":\"testing\",\"version\":\"cal_anchored\", **metrics(test_cal, \"pred_count_cal\")},\n])\nsummary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bucket_table(df, pred_col):\n    gt = df[\"gt_count\"].values\n    pred = df[pred_col].values\n    err = pred - gt\n\n    bins = [-1, 10, 30, 60, 100, 200, 10**9]\n    labels = [\"0-10\", \"11-30\", \"31-60\", \"61-100\", \"101-200\", \"200+\"]\n    b = pd.cut(gt, bins=bins, labels=labels)\n\n    out = pd.DataFrame({\"bucket\": b, \"err\": err}).groupby(\"bucket\", observed=True).agg(\n        n=(\"err\",\"count\"),\n        mae=(\"err\", lambda x: float(np.mean(np.abs(x)))),\n        rmse=(\"err\", lambda x: float(math.sqrt(np.mean(x**2)))),\n        bias=(\"err\", lambda x: float(np.mean(x))),\n    ).reset_index()\n    return out\n\nprint(\"TEST raw buckets\")\nprint(bucket_table(test, \"pred_count\").to_string(index=False))\nprint(\"\\nTEST anchored-cal buckets\")\nprint(bucket_table(test_cal, \"pred_count_cal\").to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nsrc = \"/kaggle/working/runs\"\ndst = \"/kaggle/working/runs\"  # no .zip here\nshutil.make_archive(dst, \"zip\", src)\n\nprint(dst + \".zip\")","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Publish to Hugging Face\n\nThis section packages the trained model (`best.pt`) and creates:\n- a **Model repo** (stores weights + minimal inference notes)\n- an optional **Space** (Gradio demo: upload image -> head count + annotated image)\n\nIt will need a Hugging Face access token with write permissions.\n","metadata":{}},{"cell_type":"code","source":"!pip -q install huggingface_hub gradio opencv-python\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T18:39:55.951365Z","iopub.execute_input":"2026-01-20T18:39:55.951892Z","iopub.status.idle":"2026-01-20T18:39:59.264401Z","shell.execute_reply.started":"2026-01-20T18:39:55.951860Z","shell.execute_reply":"2026-01-20T18:39:59.263611Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nfrom huggingface_hub import HfApi, login\n\n# Option A: set HF_TOKEN as a Kaggle \"Secret\" and read it from env\n# Option B: paste token here (not recommended)\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n\nhf_token = os.environ.get('HF_TOKEN')\nassert secret_value_0, 'Missing HF_TOKEN env var. Add it in Kaggle secrets or os.environ.'\nlogin(token=secret_value_0)\nprint('Logged in to Hugging Face')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T18:41:29.550756Z","iopub.execute_input":"2026-01-20T18:41:29.551471Z","iopub.status.idle":"2026-01-20T18:41:29.833527Z","shell.execute_reply.started":"2026-01-20T18:41:29.551443Z","shell.execute_reply":"2026-01-20T18:41:29.832714Z"}},"outputs":[{"name":"stdout","text":"Logged in to Hugging Face\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 1) Create a model repo and upload the weights\n\n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\nfrom pathlib import Path\n\napi = HfApi()\n\nHF_MODEL_REPO = 'AmineSam/irail-crowd-counting-yolov8n'\nLOCAL_WEIGHTS = Path('/kaggle/working/runs/detect/train/weights/best.pt')\nassert LOCAL_WEIGHTS.exists(), f'Missing weights: {LOCAL_WEIGHTS}'\n\napi.create_repo(repo_id=HF_MODEL_REPO, repo_type='model', exist_ok=True)\napi.upload_file(\n    path_or_fileobj=str(LOCAL_WEIGHTS),\n    path_in_repo='best.pt',\n    repo_id=HF_MODEL_REPO,\n    repo_type='model',\n)\n\n# Optional: upload key plots/CSVs for transparency\nfor p in [\n    Path('/kaggle/working/count_eval/scatter_test.png'),\n    Path('/kaggle/working/count_eval/error_hist_test.png'),\n]:\n    if p.exists():\n        api.upload_file(\n            path_or_fileobj=str(p),\n            path_in_repo=f\"artifacts/{p.name}\",\n            repo_id=HF_MODEL_REPO,\n            repo_type=\"model\",\n        )\n\n\nprint('Uploaded model + artifacts to:', HF_MODEL_REPO)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T18:46:11.043841Z","iopub.execute_input":"2026-01-20T18:46:11.044486Z","iopub.status.idle":"2026-01-20T18:46:14.858094Z","shell.execute_reply.started":"2026-01-20T18:46:11.044453Z","shell.execute_reply":"2026-01-20T18:46:14.857240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209caf1206a04fd4beba808f16669427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b260fcda3cc44548774621f6bc47989"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Uploaded model + artifacts to: AmineSam/irail-crowd-counting-yolov8n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2) Create a Gradio Space (optional)\n\nThis Space will:\n- load `best.pt` from the model repo\n- run YOLO inference\n- display an annotated image and the head count\n\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nSPACE_DIR = Path('/kaggle/working/hf_space')\nSPACE_DIR.mkdir(parents=True, exist_ok=True)\n\n(SPACE_DIR / 'requirements.txt').write_text('''\nimport os\nimport cv2\nimport gradio as gr\nfrom ultralytics import YOLO\nfrom huggingface_hub import hf_hub_download\n\nMODEL_REPO = \"AmineSam/irail-crowd-counting-yolov8n\"\n\n# Fixed inference params (not exposed in UI)\nDEFAULT_IMGSZ = 832\nDEFAULT_MAX_DET = 300\n\n# Crowd thresholds (based on the dataset buckets)\nLOW_MAX = 40\nMED_MAX = 100\n\n# Download weights from the model repo (public repo -> no token needed)\nweights_path = hf_hub_download(repo_id=MODEL_REPO, filename=\"best.pt\")\nmodel = YOLO(weights_path)\n\n\ndef crowd_level_from_count(count: int) -> str:\n    if count <= LOW_MAX:\n        return f\"Low (0–{LOW_MAX})\"\n    if count <= MED_MAX:\n        return f\"Medium ({LOW_MAX+1}–{MED_MAX})\"\n    return f\"High (>{MED_MAX})\"\n\n\ndef predict(image, conf=0.25, iou=0.75):\n    if image is None:\n        return None, 0, \"N/A\"\n\n    # image is RGB numpy\n    bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n    res = model.predict(\n        bgr,\n        conf=float(conf),\n        iou=float(iou),\n        imgsz=DEFAULT_IMGSZ,\n        max_det=DEFAULT_MAX_DET,\n        verbose=False,\n    )[0]\n\n    count = 0 if res.boxes is None else int(res.boxes.shape[0])\n    level = crowd_level_from_count(count)\n\n    annotated = res.plot()  # BGR annotated\n    annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n\n    return annotated, count, level\n\n\nEXAMPLES = [\n    [\"DMPG41_GP204092_frame_1500.jpg\"],\n    [\"DMSA0442_GP094171_frame_11250.jpg\"],\n    [\"DMWW71_MitsubishiElectricHalle_WincentWeiss_gopro7_1_1430to1915_part17_13680032_14535034_frame_4500.jpg\"],\n    [\"DMWW71_MitsubishiElectricHalle_WincentWeiss_gopro7_1_1430to1915_part17_13680032_14535034_frame_12000.jpg\"],\n]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# RPEE-Heads: Head detection + counting\")\n    gr.Markdown(\n        f\"**Crowd levels** (based on dataset buckets): Low ≤ {LOW_MAX}, Medium {LOW_MAX+1}–{MED_MAX}, High ≥ {MED_MAX+1}\"\n    )\n\n    with gr.Row():\n        inp = gr.Image(type=\"numpy\", label=\"Input image\")\n        out = gr.Image(type=\"numpy\", label=\"Annotated output\")\n\n    with gr.Row():\n        count = gr.Number(label=\"Predicted head count\")\n        level = gr.Textbox(label=\"Crowd level\", interactive=False)\n\n    with gr.Accordion(\"Inference settings\", open=False):\n        conf = gr.Slider(0.05, 0.80, value=0.25, step=0.05, label=\"Confidence\")\n        iou = gr.Slider(0.30, 0.90, value=0.75, step=0.05, label=\"NMS IoU\")\n\n    btn = gr.Button(\"Run\")\n    btn.click(fn=predict, inputs=[inp, conf, iou], outputs=[out, count, level])\n\n    gr.Examples(\n        examples=EXAMPLES,\n        inputs=[inp],\n        label=\"Example images\",\n    )\n\ndemo.launch()\n''')\n\nprint('Wrote Space files to:', SPACE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T18:47:52.962372Z","iopub.execute_input":"2026-01-20T18:47:52.963184Z","iopub.status.idle":"2026-01-20T18:47:52.969649Z","shell.execute_reply.started":"2026-01-20T18:47:52.963150Z","shell.execute_reply":"2026-01-20T18:47:52.968968Z"}},"outputs":[{"name":"stdout","text":"Wrote Space files to: /kaggle/working/hf_space\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from huggingface_hub import HfApi\nfrom pathlib import Path\n\napi = HfApi()\nHF_SPACE_REPO = 'AmineSam/irail-crowd-counting-yolov8n-demo'\n\napi.create_repo(repo_id=HF_SPACE_REPO, repo_type='space', space_sdk='gradio', exist_ok=True)\napi.upload_folder(folder_path=str(Path('/kaggle/working/hf_space')), repo_id=HF_SPACE_REPO, repo_type='space')\n\nprint('Uploaded Space to:', HF_SPACE_REPO)\nprint('Tip: set Space env var HF_MODEL_REPO to the model repo id.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T18:48:31.694042Z","iopub.execute_input":"2026-01-20T18:48:31.694357Z","iopub.status.idle":"2026-01-20T18:48:33.604315Z","shell.execute_reply.started":"2026-01-20T18:48:31.694328Z","shell.execute_reply":"2026-01-20T18:48:33.603673Z"}},"outputs":[{"name":"stdout","text":"Uploaded Space to: AmineSam/irail-crowd-counting-yolov8n-demo\nTip: set Space env var HF_MODEL_REPO to your model repo id.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}